{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEPS:**  \n",
    "1. Convert data to correct images size\n",
    "2. Create TrainLoader instance\n",
    "3. Define model\n",
    "4. Define hyperparameters\n",
    "5. Train model\n",
    "5. Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transform it to correct input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_xy(df: pd.DataFrame) -> torch.IntTensor and torch.IntTensor:\n",
    "    \"\"\"Converts a Pandas DataFrame to 2 torch.IntTensor (x & y) so that you can later apply any function.\n",
    "    The df needs to have shape (N, 785) where N: number of samples and 785: target + 784 pixels (28x28)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The images data you will work with\n",
    "\n",
    "    Returns:\n",
    "        torch.IntTensor: x\n",
    "        torch.IntTensor: y\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_samples = df.shape[0]\n",
    "    X_train = None\n",
    "    y_train = None\n",
    "    \n",
    "    # get the labels\n",
    "    labels = df['label'].to_numpy()\n",
    "    y_train = torch.Tensor(labels).reshape(number_of_samples, 1)\n",
    "\n",
    "    # convert the rest of dataframe to correct image size (28x28)\n",
    "    pixels = df[[col for col in df.columns if col != 'label']].to_numpy()\n",
    "    X_train = torch.Tensor(pixels).reshape(number_of_samples, 1, 28, 28)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: torch.Size([42000, 1])\n",
      "X_train shape: torch.Size([42000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = convert_df_to_xy(df=df)\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot one image to see if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_to_plot = 756\n",
    "\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(X_train[image_to_plot][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# create trainning dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "\n",
    "# create DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dSame(torch.nn.Conv2d):\n",
    "\n",
    "    def calc_same_pad(self, i: int, k: int, s: int, d: int) -> int:\n",
    "        return max((math.ceil(i / s) - 1) * s + (k - 1) * d + 1 - i, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ih, iw = x.size()[-2:]\n",
    "\n",
    "        pad_h = self.calc_same_pad(i=ih, k=self.kernel_size[0], s=self.stride[0], d=self.dilation[0])\n",
    "        pad_w = self.calc_same_pad(i=iw, k=self.kernel_size[1], s=self.stride[1], d=self.dilation[1])\n",
    "\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(\n",
    "                x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n",
    "            )\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            self.weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # in_features = channels * height * width\n",
    "        self.in_features = 1 * 28 * 28\n",
    "\n",
    "        # out_features = 1 number out of 10 numbers\n",
    "        self.out_features = 1\n",
    "\n",
    "        layer_list = [\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            Conv2dSame(in_channels=32, out_channels=32, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            Conv2dSame(in_channels=64, out_channels=64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=1024, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features=128, out_features=10),\n",
    "            nn.Softmax()\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2dSame(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.4, inplace=False)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): Conv2dSame(64, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (12): ReLU()\n",
       "    (13): Dropout(p=0.4, inplace=False)\n",
       "    (14): Flatten(start_dim=1, end_dim=-1)\n",
       "    (15): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (16): ReLU()\n",
       "    (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): Dropout(p=0.4, inplace=False)\n",
       "    (19): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (20): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0      batch: 0     loss: 1.511735439300537\n",
      "epoch: 0      batch: 10     loss: 1.4936037063598633\n",
      "epoch: 0      batch: 20     loss: 1.5161073207855225\n",
      "epoch: 0      batch: 30     loss: 1.5132595300674438\n",
      "epoch: 0      batch: 40     loss: 1.497671365737915\n",
      "epoch: 0      batch: 50     loss: 1.5034197568893433\n",
      "epoch: 0      batch: 60     loss: 1.4859261512756348\n",
      "epoch: 0      batch: 70     loss: 1.4900715351104736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/pablo/Desktop/dev/kaggle/mnist/src/torch.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pablo/Desktop/dev/kaggle/mnist/src/torch.ipynb#ch0000022?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m      batch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m     loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pablo/Desktop/dev/kaggle/mnist/src/torch.ipynb#ch0000022?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pablo/Desktop/dev/kaggle/mnist/src/torch.ipynb#ch0000022?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pablo/Desktop/dev/kaggle/mnist/src/torch.ipynb#ch0000022?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/pablo/Desktop/dev/kaggle/mnist/venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels.flatten().long())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"epoch: {epoch}      batch: {i}     loss: {loss.item()}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46764ca8b7c597d72e34090c032f2809e12fa680ea97b2b9ad0d17deccea85ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
